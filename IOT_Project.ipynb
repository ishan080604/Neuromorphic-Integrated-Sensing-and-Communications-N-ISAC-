{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishan080604/Neuromorphic-Integrated-Sensing-and-Communications-N-ISAC-/blob/main/IOT_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPEVDToIj_rs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy import signal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "L = 80  # Number of slots\n",
        "L_b = 1  # Bandwidth expansion factor\n",
        "N_c = 5  # Number of clutter paths\n",
        "SNR_dB = 10  # Signal-to-Noise Ratio in dB\n",
        "num_train_examples = 60000\n",
        "num_test_examples = 10000\n",
        "\n",
        "def generate_data(L, L_b, N_c, SNR_dB, num_examples):\n",
        "    # System parameters\n",
        "    T_c = 1  # Chip duration (normalized)\n",
        "    T = 2 * L_b * T_c  # Slot duration\n",
        "    T_h = 4 * T_c  # Maximum delay spread\n",
        "    L_h = int(T_h / T_c)  # Number of channel taps\n",
        "\n",
        "    # Generate data\n",
        "    data = []\n",
        "    for _ in range(num_examples):\n",
        "        # Generate random bits\n",
        "        x = np.random.randint(0, 2, L)\n",
        "\n",
        "        # Generate random target presence\n",
        "        v = np.random.randint(0, 2)\n",
        "\n",
        "        # Generate IR signal\n",
        "        s = np.zeros(2 * L_b * L)\n",
        "        for l in range(L):\n",
        "            if x[l] == 0:\n",
        "                s[2 * l * L_b] = 1\n",
        "            else:\n",
        "                s[(2 * l + 1) * L_b] = 1\n",
        "\n",
        "        # Generate channel\n",
        "        h = np.zeros(L_h, dtype=complex)\n",
        "        if v == 1:\n",
        "            h[0] = np.random.normal(0, 1) + 1j * np.random.normal(0, 1)\n",
        "        for _ in range(N_c):\n",
        "            delay = np.random.randint(0, L_h)\n",
        "            h[delay] += np.random.normal(0, 1) + 1j * np.random.normal(0, 1)\n",
        "\n",
        "        # Apply channel\n",
        "        y = signal.convolve(s, h, mode='same')\n",
        "\n",
        "        # Add noise\n",
        "        signal_power = np.mean(np.abs(y)**2)\n",
        "        SNR_linear = 10**(SNR_dB/10)\n",
        "        noise_power = signal_power / SNR_linear\n",
        "        noise = np.sqrt(noise_power/2) * (np.random.normal(0, 1, len(y)) + 1j * np.random.normal(0, 1, len(y)))\n",
        "        y_noisy = y + noise\n",
        "\n",
        "        # Prepare input for SNN\n",
        "        y_real = np.real(y_noisy)\n",
        "        y_imag = np.imag(y_noisy)\n",
        "        y_input = np.concatenate([y_real, y_imag])\n",
        "\n",
        "        data.append((y_input, x, v))\n",
        "\n",
        "    return data\n",
        "\n",
        "train_data = generate_data(L, L_b, N_c, SNR_dB, num_train_examples)\n",
        "test_data = generate_data(L, L_b, N_c, SNR_dB, num_test_examples)"
      ],
      "metadata": {
        "id": "RjANsfhCmY6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = []\n",
        "for i in range(0, 321, 4):\n",
        "  train_y.append(i)"
      ],
      "metadata": {
        "id": "jP743ynOn2HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_input_train = []\n",
        "x_train = []\n",
        "v_train = []\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "  for j in range(len(train_y) - 1):\n",
        "    y_input_train.append([train_data[i][0][train_y[j]:train_y[j+1]]])\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "  for j in range(L):\n",
        "    x_train.append([train_data[i][1][j]])\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "  for j in range(L):\n",
        "    v_train.append(train_data[i][2])"
      ],
      "metadata": {
        "id": "qIvcsbmiolMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_input_train = torch.tensor(y_input_train, dtype = torch.float64)\n",
        "x_train = torch.tensor(x_train, dtype = torch.float64)\n",
        "v_train = torch.tensor(v_train, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "eqxfa5W2rW6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44a1929-8ec0-43cd-f4e8-a07fc768e9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-8268a3b47bc7>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  y_input_train = torch.tensor(y_input_train, dtype = torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ISACNN(nn.Module):\n",
        "  def __init__(self, hidden_neurons):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(hidden_neurons, 1)\n",
        "    self.relu = nn.ReLU()\n",
        "    # self.flatten = nn.Flatten(start_dim=0, end_dim=-1)\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(4*L_b, hidden_neurons),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, y):\n",
        "    # y = self.flatten(y)\n",
        "    y = self.linear_relu_stack(y)\n",
        "    comm = self.linear(y)\n",
        "    sense = self.linear(y)\n",
        "    return comm, sense"
      ],
      "metadata": {
        "id": "NSlYv3OpvHEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train1 = []\n",
        "for i in range(0, (80*60000) + 1, 80):\n",
        "  train1.append(i)"
      ],
      "metadata": {
        "id": "3Fozcw_H3pZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_input = []\n",
        "x_input = []\n",
        "v_input = []\n",
        "\n",
        "for i in range(len(train1) - 1):\n",
        "  y_input.append(y_input_train[train1[i] : train1[i+1]])\n",
        "\n",
        "for i in range(len(train1) - 1):\n",
        "  x_input.append(x_train[train1[i] : train1[i+1]])\n",
        "\n",
        "for i in range(len(train1) - 1):\n",
        "  v_input.append(v_train[train1[i] : train1[i+1]])"
      ],
      "metadata": {
        "id": "wxJ_he755ByT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_input_tensor = torch.stack(y_input).reshape(60000, 80, -1)\n",
        "x_input_tensor = torch.stack(x_input)\n",
        "v_input_tensor = torch.stack(v_input)"
      ],
      "metadata": {
        "id": "-7AkmKK48sFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training ISAC**"
      ],
      "metadata": {
        "id": "KUtzcu9bI5FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "beta = 0.5\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "h = 10\n",
        "\n",
        "model = ISACNN(h)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "dataset = TensorDataset(y_input_tensor, x_input_tensor, v_input_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    total_loss = 0\n",
        "    for batch_idx, (y_batch, x_batch, v_batch) in enumerate(dataloader):\n",
        "        y_batch, x_batch, v_batch = y_batch.to(device), x_batch.to(device), v_batch.to(device)\n",
        "\n",
        "        loss_comm = 0\n",
        "        loss_sense = 0\n",
        "\n",
        "        for l in range(L):\n",
        "            comm, sense = model(y_batch[:, l, :].float())\n",
        "            loss_comm_l = criterion(comm.reshape(comm.shape[0], 1), x_batch[:, l].float())\n",
        "            loss_sense_l = criterion(sense.squeeze(), v_batch[:, l].float())\n",
        "            loss_comm += loss_comm_l\n",
        "            loss_sense += loss_sense_l\n",
        "\n",
        "        loss_comm /= 80\n",
        "        loss_sense /= 80\n",
        "        loss = (beta * loss_comm) + (1 - beta) * loss_sense\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch {batch_idx}/{len(dataloader)}: Loss: {loss.item():.4f}')\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f'Epoch {epoch+1} Average Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "id": "CipZdUht3ER3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d5622d-ed40-4dc8-c8b5-5f1cc903f746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.7196\n",
            "Batch 100/1875: Loss: 0.6906\n",
            "Batch 200/1875: Loss: 0.6970\n",
            "Batch 300/1875: Loss: 0.6950\n",
            "Batch 400/1875: Loss: 0.6951\n",
            "Batch 500/1875: Loss: 0.6953\n",
            "Batch 600/1875: Loss: 0.6930\n",
            "Batch 700/1875: Loss: 0.6934\n",
            "Batch 800/1875: Loss: 0.6940\n",
            "Batch 900/1875: Loss: 0.6944\n",
            "Batch 1000/1875: Loss: 0.6928\n",
            "Batch 1100/1875: Loss: 0.6941\n",
            "Batch 1200/1875: Loss: 0.6933\n",
            "Batch 1300/1875: Loss: 0.6894\n",
            "Batch 1400/1875: Loss: 0.6928\n",
            "Batch 1500/1875: Loss: 0.6923\n",
            "Batch 1600/1875: Loss: 0.6920\n",
            "Batch 1700/1875: Loss: 0.6905\n",
            "Batch 1800/1875: Loss: 0.6909\n",
            "Epoch 1 Average Loss: 0.6934\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6917\n",
            "Batch 100/1875: Loss: 0.6933\n",
            "Batch 200/1875: Loss: 0.6892\n",
            "Batch 300/1875: Loss: 0.6931\n",
            "Batch 400/1875: Loss: 0.6938\n",
            "Batch 500/1875: Loss: 0.6937\n",
            "Batch 600/1875: Loss: 0.6935\n",
            "Batch 700/1875: Loss: 0.6926\n",
            "Batch 800/1875: Loss: 0.6949\n",
            "Batch 900/1875: Loss: 0.6941\n",
            "Batch 1000/1875: Loss: 0.6933\n",
            "Batch 1100/1875: Loss: 0.6950\n",
            "Batch 1200/1875: Loss: 0.6938\n",
            "Batch 1300/1875: Loss: 0.6888\n",
            "Batch 1400/1875: Loss: 0.6924\n",
            "Batch 1500/1875: Loss: 0.6922\n",
            "Batch 1600/1875: Loss: 0.6920\n",
            "Batch 1700/1875: Loss: 0.6903\n",
            "Batch 1800/1875: Loss: 0.6898\n",
            "Epoch 2 Average Loss: 0.6919\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6902\n",
            "Batch 100/1875: Loss: 0.6936\n",
            "Batch 200/1875: Loss: 0.6879\n",
            "Batch 300/1875: Loss: 0.6934\n",
            "Batch 400/1875: Loss: 0.6938\n",
            "Batch 500/1875: Loss: 0.6938\n",
            "Batch 600/1875: Loss: 0.6935\n",
            "Batch 700/1875: Loss: 0.6926\n",
            "Batch 800/1875: Loss: 0.6947\n",
            "Batch 900/1875: Loss: 0.6942\n",
            "Batch 1000/1875: Loss: 0.6933\n",
            "Batch 1100/1875: Loss: 0.6952\n",
            "Batch 1200/1875: Loss: 0.6938\n",
            "Batch 1300/1875: Loss: 0.6887\n",
            "Batch 1400/1875: Loss: 0.6923\n",
            "Batch 1500/1875: Loss: 0.6921\n",
            "Batch 1600/1875: Loss: 0.6920\n",
            "Batch 1700/1875: Loss: 0.6902\n",
            "Batch 1800/1875: Loss: 0.6896\n",
            "Epoch 3 Average Loss: 0.6918\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6897\n",
            "Batch 100/1875: Loss: 0.6936\n",
            "Batch 200/1875: Loss: 0.6876\n",
            "Batch 300/1875: Loss: 0.6934\n",
            "Batch 400/1875: Loss: 0.6937\n",
            "Batch 500/1875: Loss: 0.6938\n",
            "Batch 600/1875: Loss: 0.6933\n",
            "Batch 700/1875: Loss: 0.6925\n",
            "Batch 800/1875: Loss: 0.6947\n",
            "Batch 900/1875: Loss: 0.6941\n",
            "Batch 1000/1875: Loss: 0.6932\n",
            "Batch 1100/1875: Loss: 0.6953\n",
            "Batch 1200/1875: Loss: 0.6938\n",
            "Batch 1300/1875: Loss: 0.6886\n",
            "Batch 1400/1875: Loss: 0.6923\n",
            "Batch 1500/1875: Loss: 0.6920\n",
            "Batch 1600/1875: Loss: 0.6921\n",
            "Batch 1700/1875: Loss: 0.6900\n",
            "Batch 1800/1875: Loss: 0.6894\n",
            "Epoch 4 Average Loss: 0.6918\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6894\n",
            "Batch 100/1875: Loss: 0.6935\n",
            "Batch 200/1875: Loss: 0.6874\n",
            "Batch 300/1875: Loss: 0.6934\n",
            "Batch 400/1875: Loss: 0.6936\n",
            "Batch 500/1875: Loss: 0.6937\n",
            "Batch 600/1875: Loss: 0.6931\n",
            "Batch 700/1875: Loss: 0.6925\n",
            "Batch 800/1875: Loss: 0.6947\n",
            "Batch 900/1875: Loss: 0.6941\n",
            "Batch 1000/1875: Loss: 0.6932\n",
            "Batch 1100/1875: Loss: 0.6954\n",
            "Batch 1200/1875: Loss: 0.6938\n",
            "Batch 1300/1875: Loss: 0.6886\n",
            "Batch 1400/1875: Loss: 0.6923\n",
            "Batch 1500/1875: Loss: 0.6920\n",
            "Batch 1600/1875: Loss: 0.6921\n",
            "Batch 1700/1875: Loss: 0.6900\n",
            "Batch 1800/1875: Loss: 0.6894\n",
            "Epoch 5 Average Loss: 0.6917\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6893\n",
            "Batch 100/1875: Loss: 0.6935\n",
            "Batch 200/1875: Loss: 0.6873\n",
            "Batch 300/1875: Loss: 0.6934\n",
            "Batch 400/1875: Loss: 0.6935\n",
            "Batch 500/1875: Loss: 0.6936\n",
            "Batch 600/1875: Loss: 0.6930\n",
            "Batch 700/1875: Loss: 0.6925\n",
            "Batch 800/1875: Loss: 0.6946\n",
            "Batch 900/1875: Loss: 0.6941\n",
            "Batch 1000/1875: Loss: 0.6932\n",
            "Batch 1100/1875: Loss: 0.6954\n",
            "Batch 1200/1875: Loss: 0.6938\n",
            "Batch 1300/1875: Loss: 0.6885\n",
            "Batch 1400/1875: Loss: 0.6922\n",
            "Batch 1500/1875: Loss: 0.6920\n",
            "Batch 1600/1875: Loss: 0.6921\n",
            "Batch 1700/1875: Loss: 0.6899\n",
            "Batch 1800/1875: Loss: 0.6893\n",
            "Epoch 6 Average Loss: 0.6917\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6892\n",
            "Batch 100/1875: Loss: 0.6934\n",
            "Batch 200/1875: Loss: 0.6873\n",
            "Batch 300/1875: Loss: 0.6934\n",
            "Batch 400/1875: Loss: 0.6935\n",
            "Batch 500/1875: Loss: 0.6936\n",
            "Batch 600/1875: Loss: 0.6930\n",
            "Batch 700/1875: Loss: 0.6925\n",
            "Batch 800/1875: Loss: 0.6946\n",
            "Batch 900/1875: Loss: 0.6941\n",
            "Batch 1000/1875: Loss: 0.6931\n",
            "Batch 1100/1875: Loss: 0.6954\n",
            "Batch 1200/1875: Loss: 0.6938\n",
            "Batch 1300/1875: Loss: 0.6885\n",
            "Batch 1400/1875: Loss: 0.6922\n",
            "Batch 1500/1875: Loss: 0.6920\n",
            "Batch 1600/1875: Loss: 0.6921\n",
            "Batch 1700/1875: Loss: 0.6898\n",
            "Batch 1800/1875: Loss: 0.6893\n",
            "Epoch 7 Average Loss: 0.6917\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6892\n",
            "Batch 100/1875: Loss: 0.6934\n",
            "Batch 200/1875: Loss: 0.6872\n",
            "Batch 300/1875: Loss: 0.6934\n",
            "Batch 400/1875: Loss: 0.6934\n",
            "Batch 500/1875: Loss: 0.6936\n",
            "Batch 600/1875: Loss: 0.6929\n",
            "Batch 700/1875: Loss: 0.6925\n",
            "Batch 800/1875: Loss: 0.6945\n",
            "Batch 900/1875: Loss: 0.6940\n",
            "Batch 1000/1875: Loss: 0.6931\n",
            "Batch 1100/1875: Loss: 0.6953\n",
            "Batch 1200/1875: Loss: 0.6938\n",
            "Batch 1300/1875: Loss: 0.6884\n",
            "Batch 1400/1875: Loss: 0.6921\n",
            "Batch 1500/1875: Loss: 0.6921\n",
            "Batch 1600/1875: Loss: 0.6921\n",
            "Batch 1700/1875: Loss: 0.6898\n",
            "Batch 1800/1875: Loss: 0.6893\n",
            "Epoch 8 Average Loss: 0.6917\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6892\n",
            "Batch 100/1875: Loss: 0.6934\n",
            "Batch 200/1875: Loss: 0.6872\n",
            "Batch 300/1875: Loss: 0.6934\n",
            "Batch 400/1875: Loss: 0.6934\n",
            "Batch 500/1875: Loss: 0.6936\n",
            "Batch 600/1875: Loss: 0.6929\n",
            "Batch 700/1875: Loss: 0.6924\n",
            "Batch 800/1875: Loss: 0.6945\n",
            "Batch 900/1875: Loss: 0.6940\n",
            "Batch 1000/1875: Loss: 0.6931\n",
            "Batch 1100/1875: Loss: 0.6953\n",
            "Batch 1200/1875: Loss: 0.6938\n",
            "Batch 1300/1875: Loss: 0.6884\n",
            "Batch 1400/1875: Loss: 0.6920\n",
            "Batch 1500/1875: Loss: 0.6921\n",
            "Batch 1600/1875: Loss: 0.6921\n",
            "Batch 1700/1875: Loss: 0.6897\n",
            "Batch 1800/1875: Loss: 0.6893\n",
            "Epoch 9 Average Loss: 0.6917\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6892\n",
            "Batch 100/1875: Loss: 0.6934\n",
            "Batch 200/1875: Loss: 0.6872\n",
            "Batch 300/1875: Loss: 0.6934\n",
            "Batch 400/1875: Loss: 0.6933\n",
            "Batch 500/1875: Loss: 0.6936\n",
            "Batch 600/1875: Loss: 0.6929\n",
            "Batch 700/1875: Loss: 0.6924\n",
            "Batch 800/1875: Loss: 0.6944\n",
            "Batch 900/1875: Loss: 0.6940\n",
            "Batch 1000/1875: Loss: 0.6931\n",
            "Batch 1100/1875: Loss: 0.6953\n",
            "Batch 1200/1875: Loss: 0.6938\n",
            "Batch 1300/1875: Loss: 0.6884\n",
            "Batch 1400/1875: Loss: 0.6920\n",
            "Batch 1500/1875: Loss: 0.6921\n",
            "Batch 1600/1875: Loss: 0.6921\n",
            "Batch 1700/1875: Loss: 0.6897\n",
            "Batch 1800/1875: Loss: 0.6894\n",
            "Epoch 10 Average Loss: 0.6917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating ISAC**"
      ],
      "metadata": {
        "id": "sJ_dacO5LzpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_y = []\n",
        "for i in range(0, 321, 4):\n",
        "  test_y.append(i)"
      ],
      "metadata": {
        "id": "RTlVK5rZEWH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = []\n",
        "x_test = []\n",
        "v_test = []\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "  for j in range(len(test_y) - 1):\n",
        "    y_test.append(test_data[i][0][test_y[j]:test_y[j+1]])\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "  for j in range(80):\n",
        "    x_test.append(test_data[i][1][j])\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "  for j in range(80):\n",
        "    v_test.append(test_data[i][2])"
      ],
      "metadata": {
        "id": "pypZv75GIZhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = torch.tensor(y_test, dtype = torch.float64)\n",
        "x_test = torch.tensor(x_test, dtype = torch.float64)\n",
        "v_test = torch.tensor(v_test, dtype = torch.float64)"
      ],
      "metadata": {
        "id": "a_-k1tW-J_hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = []\n",
        "for i in range(0, (80*10000) + 1, 80):\n",
        "  test.append(i)"
      ],
      "metadata": {
        "id": "wz-NBKdxLNsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_input_t = []\n",
        "x_input_t = []\n",
        "v_input_t = []\n",
        "\n",
        "for i in range(len(test) - 1):\n",
        "  y_input_t.append(y_test[test[i] : test[i+1]])\n",
        "\n",
        "for i in range(len(test) - 1):\n",
        "  x_input_t.append(x_test[test[i] : test[i+1]])\n",
        "\n",
        "for i in range(len(test) - 1):\n",
        "  v_input_t.append(v_test[test[i] : test[i+1]])"
      ],
      "metadata": {
        "id": "AxB9UIF3MCdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_input_tensor_t = torch.stack(y_input_t)\n",
        "x_input_tensor_t = torch.stack(x_input_t)\n",
        "v_input_tensor_t = torch.stack(v_input_t)"
      ],
      "metadata": {
        "id": "f77H51KpOZNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta = 0.5\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "dataset = TensorDataset(y_input_tensor_t, x_input_tensor_t, v_input_tensor_t)\n",
        "dataloader = DataLoader(dataset, batch_size = 1)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "total_loss = 0\n",
        "correct_comm = 0\n",
        "correct_sense = 0;\n",
        "\n",
        "for batch_idx, (y, x, v) in enumerate(dataloader):\n",
        "  y, x, v = y.to(device), x.to(device), v.to(device)\n",
        "\n",
        "  loss_comm_t = 0\n",
        "  loss_sense_t = 0\n",
        "\n",
        "  for l in range(y.shape[1]):\n",
        "    comm_t, sense_t = model(y[:, l, :].float())\n",
        "    loss_comm_l = criterion(comm_t, x[:, l].float().unsqueeze(1))\n",
        "    loss_sense_l = criterion(sense_t, v[:, l].float().reshape(sense_t.shape))\n",
        "    loss_comm_t += loss_comm_l\n",
        "    loss_sense_t += loss_sense_l\n",
        "\n",
        "\n",
        "  loss_comm_t /= y.shape[1]\n",
        "  loss_sense_t /= y.shape[1]\n",
        "  loss = (beta * loss_comm_t) + (1-beta) * loss_sense_t\n",
        "  total_loss += loss.item()\n",
        "\n",
        "test_loss = total_loss/(10000)\n",
        "\n",
        "\n",
        "print(f\"Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "6eqA4Vp_PAW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "527d87ed-21c0-4974-dfbd-0b088d0a57eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss: 0.692422 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Separate Sensing & Communication**"
      ],
      "metadata": {
        "id": "ibiJ-6I1tPSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Model"
      ],
      "metadata": {
        "id": "G4hSUThHuFsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SSACNN(nn.Module):\n",
        "  def __init__(self, hidden_neurons):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(hidden_neurons, 1)\n",
        "    self.relu = nn.ReLU()\n",
        "    # self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(4*L_b, hidden_neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_neurons, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, y):\n",
        "    # y = self.flatten(y)\n",
        "    y = self.linear_relu_stack(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "bm0yv9qstOaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset for training of Communication part"
      ],
      "metadata": {
        "id": "aY_IOQUlwOPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.5\n",
        "# Uses alpha*L for communication doesn't care about sensing\n",
        "# Uses L - alpha*L for sensing bits are set to 1"
      ],
      "metadata": {
        "id": "Rwb6-zFVuL7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Communication Part of SSAC"
      ],
      "metadata": {
        "id": "6OiIZMZkOpvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "h = 10\n",
        "\n",
        "model_comm = SSACNN(h)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "dataset = TensorDataset(y_input_tensor, x_input_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_comm.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    total_loss = 0\n",
        "    for batch_idx, (y_batch, x_batch) in enumerate(dataloader):\n",
        "        y_batch, x_batch= y_batch.to(device), x_batch.to(device)\n",
        "\n",
        "        loss_comm = 0\n",
        "\n",
        "        for l in range(int(alpha*L)):\n",
        "            comm = model_comm(y_batch[:, l, :].float()).squeeze()\n",
        "            comm = comm.reshape(comm.shape[0], 1)\n",
        "            loss_comm_l = criterion(comm, x_batch[:, l].float())\n",
        "            loss_comm += loss_comm_l\n",
        "\n",
        "        loss_comm /= int(alpha*L)\n",
        "        loss = loss_comm\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch {batch_idx}/{len(dataloader)}: Loss: {loss.item():.4f}')\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f'Epoch {epoch+1} Average Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C20pM3I0av4",
        "outputId": "da614799-e0f9-459d-c70a-4574db15ff14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6792\n",
            "Batch 100/1875: Loss: 0.7014\n",
            "Batch 200/1875: Loss: 0.7456\n",
            "Batch 300/1875: Loss: 0.7484\n",
            "Batch 400/1875: Loss: 0.7112\n",
            "Batch 500/1875: Loss: 0.7263\n",
            "Batch 600/1875: Loss: 0.6881\n",
            "Batch 700/1875: Loss: 0.7544\n",
            "Batch 800/1875: Loss: 0.6950\n",
            "Batch 900/1875: Loss: 0.7100\n",
            "Batch 1000/1875: Loss: 0.7193\n",
            "Batch 1100/1875: Loss: 0.7081\n",
            "Batch 1200/1875: Loss: 0.6928\n",
            "Batch 1300/1875: Loss: 0.7573\n",
            "Batch 1400/1875: Loss: 0.7074\n",
            "Batch 1500/1875: Loss: 0.6769\n",
            "Batch 1600/1875: Loss: 0.6886\n",
            "Batch 1700/1875: Loss: 0.7252\n",
            "Batch 1800/1875: Loss: 0.7272\n",
            "Epoch 1 Average Loss: 0.7122\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6792\n",
            "Batch 100/1875: Loss: 0.7014\n",
            "Batch 200/1875: Loss: 0.7456\n",
            "Batch 300/1875: Loss: 0.7484\n",
            "Batch 400/1875: Loss: 0.7112\n",
            "Batch 500/1875: Loss: 0.7263\n",
            "Batch 600/1875: Loss: 0.6881\n",
            "Batch 700/1875: Loss: 0.7544\n",
            "Batch 800/1875: Loss: 0.6950\n",
            "Batch 900/1875: Loss: 0.7100\n",
            "Batch 1000/1875: Loss: 0.7193\n",
            "Batch 1100/1875: Loss: 0.7081\n",
            "Batch 1200/1875: Loss: 0.6928\n",
            "Batch 1300/1875: Loss: 0.7573\n",
            "Batch 1400/1875: Loss: 0.7074\n",
            "Batch 1500/1875: Loss: 0.6769\n",
            "Batch 1600/1875: Loss: 0.6886\n",
            "Batch 1700/1875: Loss: 0.7252\n",
            "Batch 1800/1875: Loss: 0.7272\n",
            "Epoch 2 Average Loss: 0.7122\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6792\n",
            "Batch 100/1875: Loss: 0.7014\n",
            "Batch 200/1875: Loss: 0.7456\n",
            "Batch 300/1875: Loss: 0.7484\n",
            "Batch 400/1875: Loss: 0.7112\n",
            "Batch 500/1875: Loss: 0.7263\n",
            "Batch 600/1875: Loss: 0.6881\n",
            "Batch 700/1875: Loss: 0.7544\n",
            "Batch 800/1875: Loss: 0.6950\n",
            "Batch 900/1875: Loss: 0.7100\n",
            "Batch 1000/1875: Loss: 0.7193\n",
            "Batch 1100/1875: Loss: 0.7081\n",
            "Batch 1200/1875: Loss: 0.6928\n",
            "Batch 1300/1875: Loss: 0.7573\n",
            "Batch 1400/1875: Loss: 0.7074\n",
            "Batch 1500/1875: Loss: 0.6769\n",
            "Batch 1600/1875: Loss: 0.6886\n",
            "Batch 1700/1875: Loss: 0.7252\n",
            "Batch 1800/1875: Loss: 0.7272\n",
            "Epoch 3 Average Loss: 0.7122\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6792\n",
            "Batch 100/1875: Loss: 0.7014\n",
            "Batch 200/1875: Loss: 0.7456\n",
            "Batch 300/1875: Loss: 0.7484\n",
            "Batch 400/1875: Loss: 0.7112\n",
            "Batch 500/1875: Loss: 0.7263\n",
            "Batch 600/1875: Loss: 0.6881\n",
            "Batch 700/1875: Loss: 0.7544\n",
            "Batch 800/1875: Loss: 0.6950\n",
            "Batch 900/1875: Loss: 0.7100\n",
            "Batch 1000/1875: Loss: 0.7193\n",
            "Batch 1100/1875: Loss: 0.7081\n",
            "Batch 1200/1875: Loss: 0.6928\n",
            "Batch 1300/1875: Loss: 0.7573\n",
            "Batch 1400/1875: Loss: 0.7074\n",
            "Batch 1500/1875: Loss: 0.6769\n",
            "Batch 1600/1875: Loss: 0.6886\n",
            "Batch 1700/1875: Loss: 0.7252\n",
            "Batch 1800/1875: Loss: 0.7272\n",
            "Epoch 4 Average Loss: 0.7122\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6792\n",
            "Batch 100/1875: Loss: 0.7014\n",
            "Batch 200/1875: Loss: 0.7456\n",
            "Batch 300/1875: Loss: 0.7484\n",
            "Batch 400/1875: Loss: 0.7112\n",
            "Batch 500/1875: Loss: 0.7263\n",
            "Batch 600/1875: Loss: 0.6881\n",
            "Batch 700/1875: Loss: 0.7544\n",
            "Batch 800/1875: Loss: 0.6950\n",
            "Batch 900/1875: Loss: 0.7100\n",
            "Batch 1000/1875: Loss: 0.7193\n",
            "Batch 1100/1875: Loss: 0.7081\n",
            "Batch 1200/1875: Loss: 0.6928\n",
            "Batch 1300/1875: Loss: 0.7573\n",
            "Batch 1400/1875: Loss: 0.7074\n",
            "Batch 1500/1875: Loss: 0.6769\n",
            "Batch 1600/1875: Loss: 0.6886\n",
            "Batch 1700/1875: Loss: 0.7252\n",
            "Batch 1800/1875: Loss: 0.7272\n",
            "Epoch 5 Average Loss: 0.7122\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6792\n",
            "Batch 100/1875: Loss: 0.7014\n",
            "Batch 200/1875: Loss: 0.7456\n",
            "Batch 300/1875: Loss: 0.7484\n",
            "Batch 400/1875: Loss: 0.7112\n",
            "Batch 500/1875: Loss: 0.7263\n",
            "Batch 600/1875: Loss: 0.6881\n",
            "Batch 700/1875: Loss: 0.7544\n",
            "Batch 800/1875: Loss: 0.6950\n",
            "Batch 900/1875: Loss: 0.7100\n",
            "Batch 1000/1875: Loss: 0.7193\n",
            "Batch 1100/1875: Loss: 0.7081\n",
            "Batch 1200/1875: Loss: 0.6928\n",
            "Batch 1300/1875: Loss: 0.7573\n",
            "Batch 1400/1875: Loss: 0.7074\n",
            "Batch 1500/1875: Loss: 0.6769\n",
            "Batch 1600/1875: Loss: 0.6886\n",
            "Batch 1700/1875: Loss: 0.7252\n",
            "Batch 1800/1875: Loss: 0.7272\n",
            "Epoch 6 Average Loss: 0.7122\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6792\n",
            "Batch 100/1875: Loss: 0.7014\n",
            "Batch 200/1875: Loss: 0.7456\n",
            "Batch 300/1875: Loss: 0.7484\n",
            "Batch 400/1875: Loss: 0.7112\n",
            "Batch 500/1875: Loss: 0.7263\n",
            "Batch 600/1875: Loss: 0.6881\n",
            "Batch 700/1875: Loss: 0.7544\n",
            "Batch 800/1875: Loss: 0.6950\n",
            "Batch 900/1875: Loss: 0.7100\n",
            "Batch 1000/1875: Loss: 0.7193\n",
            "Batch 1100/1875: Loss: 0.7081\n",
            "Batch 1200/1875: Loss: 0.6928\n",
            "Batch 1300/1875: Loss: 0.7573\n",
            "Batch 1400/1875: Loss: 0.7074\n",
            "Batch 1500/1875: Loss: 0.6769\n",
            "Batch 1600/1875: Loss: 0.6886\n",
            "Batch 1700/1875: Loss: 0.7252\n",
            "Batch 1800/1875: Loss: 0.7272\n",
            "Epoch 7 Average Loss: 0.7122\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6792\n",
            "Batch 100/1875: Loss: 0.7014\n",
            "Batch 200/1875: Loss: 0.7456\n",
            "Batch 300/1875: Loss: 0.7484\n",
            "Batch 400/1875: Loss: 0.7112\n",
            "Batch 500/1875: Loss: 0.7263\n",
            "Batch 600/1875: Loss: 0.6881\n",
            "Batch 700/1875: Loss: 0.7544\n",
            "Batch 800/1875: Loss: 0.6950\n",
            "Batch 900/1875: Loss: 0.7100\n",
            "Batch 1000/1875: Loss: 0.7193\n",
            "Batch 1100/1875: Loss: 0.7081\n",
            "Batch 1200/1875: Loss: 0.6928\n",
            "Batch 1300/1875: Loss: 0.7573\n",
            "Batch 1400/1875: Loss: 0.7074\n",
            "Batch 1500/1875: Loss: 0.6769\n",
            "Batch 1600/1875: Loss: 0.6886\n",
            "Batch 1700/1875: Loss: 0.7252\n",
            "Batch 1800/1875: Loss: 0.7272\n",
            "Epoch 8 Average Loss: 0.7122\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6792\n",
            "Batch 100/1875: Loss: 0.7014\n",
            "Batch 200/1875: Loss: 0.7456\n",
            "Batch 300/1875: Loss: 0.7484\n",
            "Batch 400/1875: Loss: 0.7112\n",
            "Batch 500/1875: Loss: 0.7263\n",
            "Batch 600/1875: Loss: 0.6881\n",
            "Batch 700/1875: Loss: 0.7544\n",
            "Batch 800/1875: Loss: 0.6950\n",
            "Batch 900/1875: Loss: 0.7100\n",
            "Batch 1000/1875: Loss: 0.7193\n",
            "Batch 1100/1875: Loss: 0.7081\n",
            "Batch 1200/1875: Loss: 0.6928\n",
            "Batch 1300/1875: Loss: 0.7573\n",
            "Batch 1400/1875: Loss: 0.7074\n",
            "Batch 1500/1875: Loss: 0.6769\n",
            "Batch 1600/1875: Loss: 0.6886\n",
            "Batch 1700/1875: Loss: 0.7252\n",
            "Batch 1800/1875: Loss: 0.7272\n",
            "Epoch 9 Average Loss: 0.7122\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.6792\n",
            "Batch 100/1875: Loss: 0.7014\n",
            "Batch 200/1875: Loss: 0.7456\n",
            "Batch 300/1875: Loss: 0.7484\n",
            "Batch 400/1875: Loss: 0.7112\n",
            "Batch 500/1875: Loss: 0.7263\n",
            "Batch 600/1875: Loss: 0.6881\n",
            "Batch 700/1875: Loss: 0.7544\n",
            "Batch 800/1875: Loss: 0.6950\n",
            "Batch 900/1875: Loss: 0.7100\n",
            "Batch 1000/1875: Loss: 0.7193\n",
            "Batch 1100/1875: Loss: 0.7081\n",
            "Batch 1200/1875: Loss: 0.6928\n",
            "Batch 1300/1875: Loss: 0.7573\n",
            "Batch 1400/1875: Loss: 0.7074\n",
            "Batch 1500/1875: Loss: 0.6769\n",
            "Batch 1600/1875: Loss: 0.6886\n",
            "Batch 1700/1875: Loss: 0.7252\n",
            "Batch 1800/1875: Loss: 0.7272\n",
            "Epoch 10 Average Loss: 0.7122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "h = 10\n",
        "\n",
        "model_sense = SSACNN(h)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "dataset = TensorDataset(y_input_tensor, v_input_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_sense.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    total_loss = 0\n",
        "    for batch_idx, (y_batch, v_batch) in enumerate(dataloader):\n",
        "        y_batch, v_batch= y_batch.to(device), v_batch.to(device)\n",
        "\n",
        "        loss_sense = 0\n",
        "\n",
        "        for l in range(int(alpha*L), L):\n",
        "            sense = model_sense(y_batch[:, l, :].float())\n",
        "            loss_sense_l = criterion(sense.squeeze(), v_batch[:, l].float())\n",
        "            loss_sense += loss_sense_l\n",
        "\n",
        "        loss_sense /= L - int(alpha*L)\n",
        "        loss = loss_sense\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch {batch_idx}/{len(dataloader)}: Loss: {loss.item():.4f}')\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f'Epoch {epoch+1} Average Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkEQnlqERf-d",
        "outputId": "6f8de1f0-313d-4e63-d501-d716895491a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.7159\n",
            "Batch 100/1875: Loss: 0.7225\n",
            "Batch 200/1875: Loss: 0.7130\n",
            "Batch 300/1875: Loss: 0.6900\n",
            "Batch 400/1875: Loss: 0.7352\n",
            "Batch 500/1875: Loss: 0.6980\n",
            "Batch 600/1875: Loss: 0.6583\n",
            "Batch 700/1875: Loss: 0.7061\n",
            "Batch 800/1875: Loss: 0.7075\n",
            "Batch 900/1875: Loss: 0.7301\n",
            "Batch 1000/1875: Loss: 0.6644\n",
            "Batch 1100/1875: Loss: 0.7099\n",
            "Batch 1200/1875: Loss: 0.6792\n",
            "Batch 1300/1875: Loss: 0.6760\n",
            "Batch 1400/1875: Loss: 0.6611\n",
            "Batch 1500/1875: Loss: 0.7373\n",
            "Batch 1600/1875: Loss: 0.6735\n",
            "Batch 1700/1875: Loss: 0.6823\n",
            "Batch 1800/1875: Loss: 0.7204\n",
            "Epoch 1 Average Loss: 0.7089\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.7159\n",
            "Batch 100/1875: Loss: 0.7225\n",
            "Batch 200/1875: Loss: 0.7130\n",
            "Batch 300/1875: Loss: 0.6900\n",
            "Batch 400/1875: Loss: 0.7352\n",
            "Batch 500/1875: Loss: 0.6980\n",
            "Batch 600/1875: Loss: 0.6583\n",
            "Batch 700/1875: Loss: 0.7061\n",
            "Batch 800/1875: Loss: 0.7075\n",
            "Batch 900/1875: Loss: 0.7301\n",
            "Batch 1000/1875: Loss: 0.6644\n",
            "Batch 1100/1875: Loss: 0.7099\n",
            "Batch 1200/1875: Loss: 0.6792\n",
            "Batch 1300/1875: Loss: 0.6760\n",
            "Batch 1400/1875: Loss: 0.6611\n",
            "Batch 1500/1875: Loss: 0.7373\n",
            "Batch 1600/1875: Loss: 0.6735\n",
            "Batch 1700/1875: Loss: 0.6823\n",
            "Batch 1800/1875: Loss: 0.7204\n",
            "Epoch 2 Average Loss: 0.7089\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.7159\n",
            "Batch 100/1875: Loss: 0.7225\n",
            "Batch 200/1875: Loss: 0.7130\n",
            "Batch 300/1875: Loss: 0.6900\n",
            "Batch 400/1875: Loss: 0.7352\n",
            "Batch 500/1875: Loss: 0.6980\n",
            "Batch 600/1875: Loss: 0.6583\n",
            "Batch 700/1875: Loss: 0.7061\n",
            "Batch 800/1875: Loss: 0.7075\n",
            "Batch 900/1875: Loss: 0.7301\n",
            "Batch 1000/1875: Loss: 0.6644\n",
            "Batch 1100/1875: Loss: 0.7099\n",
            "Batch 1200/1875: Loss: 0.6792\n",
            "Batch 1300/1875: Loss: 0.6760\n",
            "Batch 1400/1875: Loss: 0.6611\n",
            "Batch 1500/1875: Loss: 0.7373\n",
            "Batch 1600/1875: Loss: 0.6735\n",
            "Batch 1700/1875: Loss: 0.6823\n",
            "Batch 1800/1875: Loss: 0.7204\n",
            "Epoch 3 Average Loss: 0.7089\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.7159\n",
            "Batch 100/1875: Loss: 0.7225\n",
            "Batch 200/1875: Loss: 0.7130\n",
            "Batch 300/1875: Loss: 0.6900\n",
            "Batch 400/1875: Loss: 0.7352\n",
            "Batch 500/1875: Loss: 0.6980\n",
            "Batch 600/1875: Loss: 0.6583\n",
            "Batch 700/1875: Loss: 0.7061\n",
            "Batch 800/1875: Loss: 0.7075\n",
            "Batch 900/1875: Loss: 0.7301\n",
            "Batch 1000/1875: Loss: 0.6644\n",
            "Batch 1100/1875: Loss: 0.7099\n",
            "Batch 1200/1875: Loss: 0.6792\n",
            "Batch 1300/1875: Loss: 0.6760\n",
            "Batch 1400/1875: Loss: 0.6611\n",
            "Batch 1500/1875: Loss: 0.7373\n",
            "Batch 1600/1875: Loss: 0.6735\n",
            "Batch 1700/1875: Loss: 0.6823\n",
            "Batch 1800/1875: Loss: 0.7204\n",
            "Epoch 4 Average Loss: 0.7089\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.7159\n",
            "Batch 100/1875: Loss: 0.7225\n",
            "Batch 200/1875: Loss: 0.7130\n",
            "Batch 300/1875: Loss: 0.6900\n",
            "Batch 400/1875: Loss: 0.7352\n",
            "Batch 500/1875: Loss: 0.6980\n",
            "Batch 600/1875: Loss: 0.6583\n",
            "Batch 700/1875: Loss: 0.7061\n",
            "Batch 800/1875: Loss: 0.7075\n",
            "Batch 900/1875: Loss: 0.7301\n",
            "Batch 1000/1875: Loss: 0.6644\n",
            "Batch 1100/1875: Loss: 0.7099\n",
            "Batch 1200/1875: Loss: 0.6792\n",
            "Batch 1300/1875: Loss: 0.6760\n",
            "Batch 1400/1875: Loss: 0.6611\n",
            "Batch 1500/1875: Loss: 0.7373\n",
            "Batch 1600/1875: Loss: 0.6735\n",
            "Batch 1700/1875: Loss: 0.6823\n",
            "Batch 1800/1875: Loss: 0.7204\n",
            "Epoch 5 Average Loss: 0.7089\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.7159\n",
            "Batch 100/1875: Loss: 0.7225\n",
            "Batch 200/1875: Loss: 0.7130\n",
            "Batch 300/1875: Loss: 0.6900\n",
            "Batch 400/1875: Loss: 0.7352\n",
            "Batch 500/1875: Loss: 0.6980\n",
            "Batch 600/1875: Loss: 0.6583\n",
            "Batch 700/1875: Loss: 0.7061\n",
            "Batch 800/1875: Loss: 0.7075\n",
            "Batch 900/1875: Loss: 0.7301\n",
            "Batch 1000/1875: Loss: 0.6644\n",
            "Batch 1100/1875: Loss: 0.7099\n",
            "Batch 1200/1875: Loss: 0.6792\n",
            "Batch 1300/1875: Loss: 0.6760\n",
            "Batch 1400/1875: Loss: 0.6611\n",
            "Batch 1500/1875: Loss: 0.7373\n",
            "Batch 1600/1875: Loss: 0.6735\n",
            "Batch 1700/1875: Loss: 0.6823\n",
            "Batch 1800/1875: Loss: 0.7204\n",
            "Epoch 6 Average Loss: 0.7089\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.7159\n",
            "Batch 100/1875: Loss: 0.7225\n",
            "Batch 200/1875: Loss: 0.7130\n",
            "Batch 300/1875: Loss: 0.6900\n",
            "Batch 400/1875: Loss: 0.7352\n",
            "Batch 500/1875: Loss: 0.6980\n",
            "Batch 600/1875: Loss: 0.6583\n",
            "Batch 700/1875: Loss: 0.7061\n",
            "Batch 800/1875: Loss: 0.7075\n",
            "Batch 900/1875: Loss: 0.7301\n",
            "Batch 1000/1875: Loss: 0.6644\n",
            "Batch 1100/1875: Loss: 0.7099\n",
            "Batch 1200/1875: Loss: 0.6792\n",
            "Batch 1300/1875: Loss: 0.6760\n",
            "Batch 1400/1875: Loss: 0.6611\n",
            "Batch 1500/1875: Loss: 0.7373\n",
            "Batch 1600/1875: Loss: 0.6735\n",
            "Batch 1700/1875: Loss: 0.6823\n",
            "Batch 1800/1875: Loss: 0.7204\n",
            "Epoch 7 Average Loss: 0.7089\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.7159\n",
            "Batch 100/1875: Loss: 0.7225\n",
            "Batch 200/1875: Loss: 0.7130\n",
            "Batch 300/1875: Loss: 0.6900\n",
            "Batch 400/1875: Loss: 0.7352\n",
            "Batch 500/1875: Loss: 0.6980\n",
            "Batch 600/1875: Loss: 0.6583\n",
            "Batch 700/1875: Loss: 0.7061\n",
            "Batch 800/1875: Loss: 0.7075\n",
            "Batch 900/1875: Loss: 0.7301\n",
            "Batch 1000/1875: Loss: 0.6644\n",
            "Batch 1100/1875: Loss: 0.7099\n",
            "Batch 1200/1875: Loss: 0.6792\n",
            "Batch 1300/1875: Loss: 0.6760\n",
            "Batch 1400/1875: Loss: 0.6611\n",
            "Batch 1500/1875: Loss: 0.7373\n",
            "Batch 1600/1875: Loss: 0.6735\n",
            "Batch 1700/1875: Loss: 0.6823\n",
            "Batch 1800/1875: Loss: 0.7204\n",
            "Epoch 8 Average Loss: 0.7089\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.7159\n",
            "Batch 100/1875: Loss: 0.7225\n",
            "Batch 200/1875: Loss: 0.7130\n",
            "Batch 300/1875: Loss: 0.6900\n",
            "Batch 400/1875: Loss: 0.7352\n",
            "Batch 500/1875: Loss: 0.6980\n",
            "Batch 600/1875: Loss: 0.6583\n",
            "Batch 700/1875: Loss: 0.7061\n",
            "Batch 800/1875: Loss: 0.7075\n",
            "Batch 900/1875: Loss: 0.7301\n",
            "Batch 1000/1875: Loss: 0.6644\n",
            "Batch 1100/1875: Loss: 0.7099\n",
            "Batch 1200/1875: Loss: 0.6792\n",
            "Batch 1300/1875: Loss: 0.6760\n",
            "Batch 1400/1875: Loss: 0.6611\n",
            "Batch 1500/1875: Loss: 0.7373\n",
            "Batch 1600/1875: Loss: 0.6735\n",
            "Batch 1700/1875: Loss: 0.6823\n",
            "Batch 1800/1875: Loss: 0.7204\n",
            "Epoch 9 Average Loss: 0.7089\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Batch 0/1875: Loss: 0.7159\n",
            "Batch 100/1875: Loss: 0.7225\n",
            "Batch 200/1875: Loss: 0.7130\n",
            "Batch 300/1875: Loss: 0.6900\n",
            "Batch 400/1875: Loss: 0.7352\n",
            "Batch 500/1875: Loss: 0.6980\n",
            "Batch 600/1875: Loss: 0.6583\n",
            "Batch 700/1875: Loss: 0.7061\n",
            "Batch 800/1875: Loss: 0.7075\n",
            "Batch 900/1875: Loss: 0.7301\n",
            "Batch 1000/1875: Loss: 0.6644\n",
            "Batch 1100/1875: Loss: 0.7099\n",
            "Batch 1200/1875: Loss: 0.6792\n",
            "Batch 1300/1875: Loss: 0.6760\n",
            "Batch 1400/1875: Loss: 0.6611\n",
            "Batch 1500/1875: Loss: 0.7373\n",
            "Batch 1600/1875: Loss: 0.6735\n",
            "Batch 1700/1875: Loss: 0.6823\n",
            "Batch 1800/1875: Loss: 0.7204\n",
            "Epoch 10 Average Loss: 0.7089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Communication part of SSAC"
      ],
      "metadata": {
        "id": "OdVZAIkSU33M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model_comm.eval()\n",
        "\n",
        "dataset = TensorDataset(y_input_tensor_t, x_input_tensor_t)\n",
        "dataloader = DataLoader(dataset, batch_size = 1)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_comm.to(device)\n",
        "\n",
        "total_loss = 0\n",
        "correct_comm = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (y, x) in enumerate(dataloader):\n",
        "    y, x = y.to(device), x.to(device)\n",
        "\n",
        "    loss_comm_t = 0\n",
        "\n",
        "    for l in range(int(alpha * L)):\n",
        "      comm_t = model_comm(y[:, l, :].float())\n",
        "      loss_comm_l = criterion(comm_t, x[:, l].float().unsqueeze(1))\n",
        "      loss_comm_t += loss_comm_l\n",
        "\n",
        "    loss_comm_t /= int(alpha*L)\n",
        "    loss = loss_comm_t\n",
        "    total_loss += loss.item()\n",
        "\n",
        "test_loss = total_loss/10000\n",
        "print(f\"Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhEm5hnQTnvh",
        "outputId": "8f4f9eee-c7b9-4c7a-f26c-a125663973d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss: 0.712137 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model_sense.eval()\n",
        "\n",
        "dataset = TensorDataset(y_input_tensor_t, v_input_tensor_t)\n",
        "dataloader = DataLoader(dataset, batch_size = 1)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_sense.to(device)\n",
        "\n",
        "total_loss = 0\n",
        "correct_sense = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (y, v) in enumerate(dataloader):\n",
        "    y, v = y.to(device), v.to(device)\n",
        "\n",
        "    loss_sense_t = 0\n",
        "\n",
        "    for l in range(int(alpha * L), L):\n",
        "      sense_t = model_sense(y[:, l, :].float())\n",
        "      loss_sense_l = criterion(sense_t, v[:, l].float().unsqueeze(1))\n",
        "      loss_sense_t += loss_sense_l\n",
        "\n",
        "    loss_sense_t /= L - int(alpha*L)\n",
        "    loss = loss_sense_t\n",
        "    total_loss += loss.item()\n",
        "\n",
        "test_loss = total_loss/10000\n",
        "print(f\"Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgSySrF5XMu1",
        "outputId": "c1739c24-b128-4b1b-a5c2-759a68ef9d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss: 0.709362 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}